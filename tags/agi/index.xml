<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AGI on Learning is living</title><link>https://krawczuk.eu/tags/agi/</link><description>Recent content in AGI on Learning is living</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Tue, 17 Apr 2018 08:27:59 +0000</lastBuildDate><atom:link href="https://krawczuk.eu/tags/agi/index.xml" rel="self" type="application/rss+xml"/><item><title>My Position on AI Risk and superintelligence</title><link>https://krawczuk.eu/opinion/mypositiononairisk/</link><pubDate>Tue, 17 Apr 2018 08:27:59 +0000</pubDate><guid>https://krawczuk.eu/opinion/mypositiononairisk/</guid><description>&amp;hellip;can be best summarized by this xkcd and this keynote by Charles Stross. Maciej Ceg≈Çowski also has some good stuff.
In my own words: I think AGI risk in the sense of alignment and controllability is an interesting field of research, but I also think that alignment is identical or smaller than the problem of governance in politics control is identical or smaller than the problem of controllability of agent based optimization algorithms, two examples being society and capitalism superintelligence is a red herring, human misuse of AI is a problem Why do I think that superintelligence/AGI is not a problem?</description></item></channel></rss>