<!doctype html><html lang=en-uk>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<title>My Position on AI Risk and superintelligence - Learning is living</title>
<meta name=description content="&mldr;can be best summarized by this xkcd and this keynote by Charles Stross. Maciej Cegłowski also has some good stuff.
In my own words:  I think AGI risk in the sense of alignment and controllability is an interesting field of research, but I also think that  alignment is identical or smaller than the problem of governance in politics control is identical or smaller than the problem of controllability of agent based optimization algorithms, two examples being society and capitalism superintelligence is a red herring, human misuse of AI is a problem   Why do I think that superintelligence/AGI is not a problem?">
<meta name=author content="Igor Krawczuk">
<link href="https://fonts.googleapis.com/css?family=PT+Sans:400,400i,700,700i" rel=stylesheet>
<link href=https://cdn.bootcss.com/highlight.js/9.12.0/styles/default.min.css rel=stylesheet>
<link href=https://krawczuk.eu/css/style.css rel=stylesheet>
<link rel=apple-touch-icon href=https://krawczuk.eu/img/apple-touch-icon.png>
<link rel=icon href=https://krawczuk.eu/img/favicon.ico>
<meta name=generator content="Hugo 0.87.0">
<link rel=alternate type=application/atom+xml href=https://krawczuk.eu/index.xml title="Learning is living">
</head>
<body class=single>
<header class=header>
<p class=title><a href=https://krawczuk.eu/>Learning is living</a></p>
<button class=menu-toggle type=button></button>
<nav class=menu>
<ul>
<li>
<a href=/>Home</a>
</li>
<li>
<a href=/articles/>Articles</a>
</li>
<li>
<a href=/blog/>Blog</a>
</li>
<li>
<a href=/opinions/>Collection of opinions and positions</a>
</li>
<li>
<a href=/projects/>Some of my projects</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class="post post-view">
<header class=post-header>
<h1 class=post-title>My Position on AI Risk and superintelligence</h1>
<p class=post-meta>Igor Krawczuk · 2018.04.17 Last modified: 2018.08.03
</p>
</header>
<div class=post-content><p>&mldr;<a href=https://m.xkcd.com/1968/>can be best summarized by this xkcd</a> and this <a href=http://www.antipope.org/charlie/blog-static/2018/01/dude-you-broke-the-future.html>keynote by Charles Stross</a>. <a href="https://news.ycombinator.com/item?id=16822376">Maciej Cegłowski</a> also has some good stuff.</p>
<h1 id=in-my-own-words>In my own words:</h1>
<ul>
<li>I think AGI risk in the sense of alignment and controllability is an interesting field of research, <strong>but</strong> I also think that
<ul>
<li>alignment is identical or smaller than the problem of governance in politics</li>
<li>control is identical or smaller than the problem of controllability of agent based optimization algorithms, two examples being society and capitalism</li>
<li>superintelligence is a red herring, human misuse of AI is a problem</li>
</ul>
</li>
<li>Why do I think that superintelligence/AGI is not a problem?
<ul>
<li>silicon AI is right now mainly platform and physics limited, not algorithm limited, and the exponential compute growth has been fought for with more and more capital investment</li>
<li>there are also inherent limits to computation</li>
<li>we are thus either WAY past &ldquo;takeoff&rdquo; (if you don&rsquo;t constrain yourself to silicon AI) or we do not need to worry about takeoff (if you do) because of the mentioned limits</li>
</ul>
</li>
<li>Why do so many people still worry about AGI? In my opinion, because
<ul>
<li>it is interesting</li>
<li>it is less messy than boring old problems like &ldquo;good governance&rdquo;, &ldquo;moral decision making in a diverse world&rdquo; or &ldquo;dealing with the balance between capitalism and humanism&rdquo;</li>
<li>it has the allure of a few smart people saving the world from calamity (one of which might be YOU. Yes, you reading this text!)</li>
<li>the tech community sometimes thinks they are less affected by human need for spirituality and religion, but they are wrong. It thus has a history of techno utopian pseudo religions, and this is another incarnation</li>
<li>(very lightly) you can make a good living as an academic, author and intellectual of profiting of this type of techno optimism mixed with techno fear. I don&rsquo;t like this reasoning to much because it is a bit ad hominem, but there is some validity to it.It is hard to be critical of something your paycheck depends on. I am convinced most of the proponents are honest believers,though and I am glad people exist who take this seriously. Its just important to push back as well because&mldr;</li>
<li>&mldr;I also think it is a nice red herring away from hard questions we would rather ignore dealing with like
<ul>
<li>do we want a totally quantified and algorithmified society?</li>
<li>if we go through another industrial revolution, can we avoid the suffering that comes from a large chunk of workers (not all of them, just the non-retrainable chunk) becoming &ldquo;obsolete&rdquo;?</li>
<li>how do we deal with mass surveillance? how do we deal with the problem of robot police in authoritarian governments? how do we feel about drone assassinations?</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id=some-stuff-i-do-find-very-worthwhile>Some stuff I do find very worthwhile</h1>
<ul>
<li>research on misuse of AI, like the <a href=http://maliciousaireport.com/>malicious AI report</a></li>
<li>technical AI safety with a focus on current approaches, i.e.research on how to make current AI more robust and safe against accidents, like the work by some fellow <a href=https://papers.nips.cc/paper/6618-dynamic-safe-interruptibility-for-decentralized-multi-agent-reinforcement-learning.pdf>EPFL researchers</a> on safe interruptibility</li>
<li>the research into fair training to combat the entrenchment of biases into AI models</li>
<li>the use of adversarial AI as motivator to solve more complex game theoretical settings</li>
<li>ongoing work in error bounds,differential privacy, interpretability etc.</li>
<li>work to estimate and mitigate the political and economic changes coming from AI, as to preserve the civil liberties and freedoms we came to enjoy(especially in western europe) between 1980 and 2010</li>
</ul>
<h1 id=more-to-come>More to come</h1>
<p>This is it in a nutshell, but I feel I should give some arguments and citations to justify my perspective, so I will update this post over time to add those.
But this should give you something tangible to argue against if you want to convince or correct me:-)</p>
<h2 id=as-promised>As promised:</h2>
<ul>
<li>Why I am skeptical of the feasibility of runaway superintelligence: <a href=https://nbviewer.jupyter.org/urls/gitlab.com/igor-krawczuk/notebooks-public/raw/master/The%20power%20and%20necessity%20of%20imperfection.ipynb>The power and necessity of imperfection</a>. Note how this argument strengthens need for technical AI safety like done by El Mhamdi et.al, and the work done on interpretability since the imperfections would mean danger if a faulty system is put into production</li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://krawczuk.eu/tags/ai-risk/>AI risk</a></li>
<li><a href=https://krawczuk.eu/tags/superintelligence/>superintelligence</a></li>
<li><a href=https://krawczuk.eu/tags/agi/>AGI</a></li>
<li><a href=https://krawczuk.eu/tags/politics/>politics</a></li>
</ul>
</footer>
</article>
</main>
<footer class=footer>
<p><a href=https://krawczuk.eu/index.xml>Atom feed</a> | Other web presences:
<a href=mailto:contact%20%28at%29%20krawczuk%20%28point%29%20eu target=_blank>Email</a>
<a href=https://github.com/igor-krawczuk target=_blank>GitHub</a>
<a href=https://gitlab.com/igor-krawczuk target=_blank>GitLab</a>
<a href=https://twitter.com/thegermanpole target=_blank>Twitter</a>
</p>
<span>&copy;2016, 2021 Igor Krawczuk </span>
</footer>
<script src=https://cdn.bootcss.com/instantclick/3.0.1/instantclick.min.js data-no-instant></script>
<script data-no-instant>InstantClick.init()</script>
<script src=https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js data-no-instant></script>
<script data-no-instant>hljs.initHighlightingOnLoad(),addMenuListener(),InstantClick.on('change',function(){for(var b=document.querySelectorAll('pre code'),a=0;a<b.length;a++)hljs.highlightBlock(b[a]);addMenuListener()});function addMenuListener(){var a=document.querySelector('.menu-toggle'),b=document.querySelector('body');a.addEventListener('click',function(){b.classList.toggle('noscroll')},!1)}</script>
</body>
</html>